/*
 * Copyright (c) 2007-2013 Concurrent, Inc. All Rights Reserved.
 *
 * Project and contact information: http://www.cascading.org/
 *
 * This file is part of the Cascading project.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
import java.security.MessageDigest
import java.text.SimpleDateFormat

import org.apache.tools.ant.filters.ReplaceTokens

apply from: 'etc/version.gradle'
apply from: 'etc/s3Upload.gradle'
apply from: 'etc/dependencyVersions.gradle'

buildscript {
  repositories {
    mavenLocal()
    mavenCentral()
    maven { url 'http://repo.springsource.org/plugins-release' }
  }
  dependencies {
    classpath 'org.springframework.build.gradle:propdeps-plugin:0.0.4'
    classpath 'eu.appsatori:gradle-fatjar-plugin:0.2-rc1'

  }
}

// gives 'provided' scope for hadoop
configure( allprojects ) {
  apply plugin: 'propdeps'
  apply plugin: 'propdeps-maven'
  apply plugin: 'propdeps-idea'
  apply plugin: 'propdeps-eclipse'
}


ext.buildTimestamp = new SimpleDateFormat( "yyyy-MM-dd HH:mm:ss" ).format(
        new Date() )
ext.buildDate = new SimpleDateFormat( "yyyyMMdd" ).format( new Date() )

ext.tarBaseName = "${rootProject.name}-${buildDate}"
ext.finalTarName = "${tarBaseName}.tgz"
ext.assemblyDirName = "build/assemble"

// set property to use locally installed cascading builds
// otherwise wip releases will be pulled from conjars
if( System.properties[ 'cascading.release.local' ] )
  cascadingVersion = '2.5.1-wip-dev'



subprojects {
  ext.jarName = "${project.name}-${buildDate}.jar"
  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'idea'
  apply plugin: 'eclipse'

  repositories{
    mavenLocal()
    mavenCentral()
    mavenRepo name: 'conjars', url: 'http://conjars.org/repo/'
  }

  artifacts {
    archives jar
  }

  dependencies {
    testCompile group: 'junit', name: 'junit', version: "4.11"
  }
}


task assembleTarballStructure( ) << {
  def targetDirName = "${assemblyDirName}/${rootProject.name}-${buildDate}"
  copy {
    from( "src/main/shell" )
      filter( ReplaceTokens, tokens: [
              'location': project.s3Bucket.toString(),
              'majorVersion': majorVersion.toString()
      ] ) 
    into( "${targetDirName}/bin" )
  }
  copy {
    from( "src/main/resources/data" )
    into( "${targetDirName}/data" )
  }
  copy {
    from( "." )
    include( "apl.txt", "README.md" )
    into( targetDirName )
  }
  copy {
    from( "multitool-local/build/libs" )
    into( "${targetDirName}/platform/local" )
  }
  copy {
    from( "multitool-hadoop/build/libs" )
    into( "${targetDirName}/platform/hadoop" )
  }
  copy {
    from( "multitool-hadoop2-mr1/build/libs" )
    into( "${targetDirName}/platform/hadoop2-mr1" )
  }
  copy {
    from ( "multitool-docs/build/asciidoc" )
    into( "${targetDirName}/docs" )
      include( "*.html" )
    }
}

assembleTarballStructure.dependsOn( [":multitool-docs:asciidoctor", ":multitool-hadoop:jar", ":multitool-hadoop2-mr1:jar", ":multitool-local:fatJar" ] )

task buildTarball( type: Tar, dependsOn: assembleTarballStructure ) {
  baseName = "${tarBaseName}"
  destinationDir = new File( "${s3UploadArtifacts.source}" )
  compression = Compression.GZIP
  from( assemblyDirName )
}

task checksums( dependsOn: buildTarball ) << {
  generateChecksumFiles( buildTarball.archivePath, [ "sha1", "md5" ] )
}

task assembleDist( dependsOn: checksums ) << {
  file( "${s3UploadArtifacts.source}/latest.txt" ).write( "http://${s3UploadArtifacts.destination}${finalTarName}" )
  file( "${s3UploadArtifacts.source}/latest-sha1.txt" ).write( "http://${s3UploadArtifacts.destination}${finalTarName}.sha1" )
  file( "${s3UploadArtifacts.source}/latest-md5.txt" ).write( "http://${s3UploadArtifacts.destination}${finalTarName}.md5" )
  file( "${s3UploadArtifacts.source}/latest-hadoop-jar.txt" ).write( "http://${s3UploadArtifacts.destination}${project( 'multitool-hadoop').jarName}" )
  file( "${s3UploadArtifacts.source}/latest-hadoop2-mr1-jar.txt" ).write( "http://${s3UploadArtifacts.destination}${project( 'multitool-hadoop2-mr1').jarName}" )
  copy {
    from ( "multitool-hadoop/build/libs/" )
    into( "${s3UploadArtifacts.source}" )
    include( "*jar" )
  }
  copy {
    from ( "multitool-hadoop2-mr1/build/libs/" )
    into( "${s3UploadArtifacts.source}" )
    include( "*jar" )
  }
  copy {
    from( "src/main/shell/util/install-multitool.sh" )
      filter( ReplaceTokens, tokens: [
              'location': project.s3Bucket.toString(),
              'majorVersion': majorVersion.toString()
      ] ) 
    into( "${s3UploadArtifacts.source}" )
  }
}

assembleDist.dependsOn( [ "build", ":multitool-hadoop:jar", ":multitool-hadoop2-mr1:jar" ] )

def generateChecksumFiles( filePath, algorithms ){
  def mb = 1024 * 1024
  def digests = algorithms.collect{ it -> MessageDigest.getInstance( it ) }

  def inputFile = file( filePath )
  inputFile.eachByte( mb ) { byte[] buf, int bytesRead ->
    digests.each{ it -> it.update( buf, 0 , bytesRead ) }
  }

  algorithms.eachWithIndex(){ it, index ->
    file( "${filePath}.${it}"  ).write(
        new BigInteger( 1, digests.get( index ).digest() ).toString( 16 ) +
        "  " + file( filePath ).name + "\n" )
  }
}

idea {
  module {
    downloadJavadoc = true
    downloadSources = true
  }
}

eclipse {
  classpath {
    defaultOutputDir = file( 'build' )
    downloadSources = true
    downloadJavadoc = true
  }
}
